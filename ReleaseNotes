Changes from Version 2 to Version 3 alpha:

Features:
-A set of perl wrapper is now included.  New tools include

a, lm_train, a tool for one step LM training. User could configure the
training using an XML file. 
b, ngram_interp, a convenientn wrapper for interpolating LMs, an
automatic process is also included and allow users to trade off
perplexity and OOVs.
c, ngram_pronounce, generate pronunciation dictionary from one or more
language models
d, ngram_train, train an N-gram language model
e, ngram_test, test an N-gram language model
f, class_tagger.pl, tag word with class tags.  This will allow
training of class-based LM.
g, build_vocab, build a set of vocabulary from some transcripts. 
h, normalize_test, produce transcripts for acoustic model training and
evaluation

These tools significantly simplify the training process of LM and its
interaction with the AM training process. 

-In the code level, changes include
a, The toolkit now supports more than 65536 unique words in the corpora. 
Empirical test shows the toolkit works for a corpora with 10M unique words. 
b, lm_combine, a tool that could make use of weight files generated by
interp to combine LMs. 
c, text generation using trigram is now supported. 
d, Modified Kneyser-Ney smoothing (contributed by Professor Yanick
Esteve in LIUM) Currently the code only exist in the branch of the
code. We will incorporate it soon. 

For Developers:
-Version 3 alpha will be relicenced under the liberal BSD clauses. 
-The code has been mildly re-written and re-factored. 
-./autogen.sh -> configure compilation is now added. 
-The toolkit, including the perl script and C-code are now regression
tested

Version 3 Authors:
Alan Black (Advisor)
Arthur Chan (Developers)
David Huggins-Daines (Developers)
Alex Rudnicky (Advisor)

Contributors: (in alphabetical order)
Ananlada Chotimongkol
Yanick Esteve from LIUM 
Arthur Toth
Wen Xu
