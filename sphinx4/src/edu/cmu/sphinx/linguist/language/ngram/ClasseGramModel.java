/*
 * Copyright 1999-2002 Carnegie Mellon University.  
 * Portions Copyright 2002 Sun Microsystems, Inc.  
 * Portions Copyright 2002 Mitsubishi Electric Research Laboratories.
 * All Rights Reserved.  Use is subject to license terms.
 * 
 * See the file "license.terms" for information on usage and
 * redistribution of this file, and for a DISCLAIMER OF ALL 
 * WARRANTIES.
 *
 */

package edu.cmu.sphinx.linguist.language.ngram;
import java.util.Scanner;
import java.io.FileOutputStream;
import java.io.IOException;
import java.io.PrintWriter;
import java.util.Arrays;
import java.util.Collections;
import java.util.HashMap;
import java.util.HashSet;
import java.util.LinkedHashMap;
import java.util.Map;
import java.util.Set;
import java.util.logging.Logger;
import java.util.logging.Level;
import java.io.File;

import edu.cmu.sphinx.linguist.WordSequence;
import edu.cmu.sphinx.linguist.dictionary.Dictionary;
import edu.cmu.sphinx.linguist.dictionary.Word;
import edu.cmu.sphinx.linguist.language.ngram.LanguageModel;
import edu.cmu.sphinx.util.LogMath;
import edu.cmu.sphinx.util.Timer;
import edu.cmu.sphinx.util.props.PropertyException;
import edu.cmu.sphinx.util.props.PropertySheet;
import edu.cmu.sphinx.util.props.PropertyType;
import edu.cmu.sphinx.util.props.Registry;

/**
 * Queries a binary language model file generated by the 
 * <a href="http://www.speech.cs.cmu.edu/SLM_info.html">
 * CMU-Cambridge Statistical Language Modelling Toolkit</a>.
 * 
 * Note that all probabilites in the grammar are stored in LogMath log base
 * format. Language Probabilties in the language model file are stored in log
 * 10 base. They are converted to the LogMath logbase.
 */
public class ClasseGramModel implements LanguageModel {
    /**
     * Sphinx property for the name of the file that logs all the queried
     * N-grams. If this property is set to null, it means that the queried
     * N-grams are not logged.
     */
    public static final String PROP_QUERY_LOG_FILE = "queryLogFile";

    /**
     * The default value for PROP_QUERY_LOG_FILE.
     */
    public static final String PROP_QUERY_LOG_FILE_DEFAULT = null;

    /**
     * A sphinx property that defines that maxium number of trigrams to be
     * cached
     */
    public static final String PROP_CACHE_SIZE = "cacheSize";

    /**
     * The default value for the PROP_TRIGRAM_CACHE_SIZE property
     */
    public static final int PROP_CACHE_SIZE_DEFAULT = 100000;


    /**
     * A sphinx property that controls whether the bigram and trigram caches
     * are cleared after every utterance
     */
    public static final String PROP_CLEAR_CACHES_AFTER_UTTERANCE = "clearCachesAfterUtterance";

    /**
     * The default value for the PROP_CLEAR_CACHES_AFTER_UTTERANCE property
     */
    public static final boolean PROP_CLEAR_CACHES_AFTER_UTTERANCE_DEFAULT = false;

    /**
     * Sphinx property that defines the language weight for the search
     */
    public final static String PROP_LANGUAGE_WEIGHT = "languageWeight";

    /**
     * The default value for the PROP_LANGUAGE_WEIGHT property
     */
    public final static float PROP_LANGUAGE_WEIGHT_DEFAULT = 1.0f;
    /**
     * Sphinx property that defines the logMath component.
     */
    public final static String PROP_LOG_MATH = "logMath";

    /**
     * Sphinx propert that controls whether or not the language model will
     * apply the language weight and word insertion probability
     */
    public final static String PROP_APPLY_LANGUAGE_WEIGHT_AND_WIP = "applyLanguageWeightAndWip";

    /**
     * The default value for PROP_APPLY_LANGUAGE_WEIGHT_AND_WIP
     */
    public final static boolean PROP_APPLY_LANGUAGE_WEIGHT_AND_WIP_DEFAULT = false;

    /**
     * Word insertion probability property
     */
    public final static String PROP_WORD_INSERTION_PROBABILITY = "wordInsertionProbability";

    /**
     * The default value for PROP_WORD_INSERTION_PROBABILITY
     */
    public final static double PROP_WORD_INSERTION_PROBABILITY_DEFAULT = 1.0;


    /**
      * If true, use full bigram information to determine smear
      */
    public final static String PROP_FULL_SMEAR = "fullSmear";

    /**
      * Default value for PROP_FULL_SMEAR
      */
    public final static boolean PROP_FULL_SMEAR_DEFAULT = false;

    /**
     * The number of bytes per bigram in the LM file generated by the
     * CMU-Cambridge Statistical Language Modelling Toolkit.
     */
  public final static String PROP_DICTIONARY_CLASSE = "dictionaryClasse";


    /**
     * The number of bytes per trigram in the LM file generated by the
     * CMU-Cambridge Statistical Language Modelling Toolkit.
     */
    // ------------------------------
    // Configuration data
    // ------------------------------
    private Logger logger;
    private LogMath logMath;
    private String name;
    private String ngramLogFile;
    private int maxCacheSize;
    private boolean clearCacheAfterUtterance;
    private boolean fullSmear;
    private int maxDepth;
    private Dictionary dictionary;
    private Dictionary dictionaryClasse;
    private LanguageModel wordLM;
    private LanguageModel basicLM;
    private LanguageModel classeLM;
    private String format;
    private File location;
    private boolean applyLanguageWeightAndWip;
    private float languageWeight;
    private double wip;
    private float unigramWeight;
    private float poids;
    // -------------------------------
    // Statistics
    // -------------------------------
    private int misses;
    private int hit;
    private int smearTermCount = 0;

    // -------------------------------
    // subcomponents
    // --------------------------------

    private PrintWriter logFile;

    // -------------------------------
    // Working data
    // --------------------------------
    private Map unigramIDMap;
    private LRUCache cache;
    private Map bigramSmearMap;
    
    /*
     * (non-Javadoc)
     * 
     * @see edu.cmu.sphinx.util.props.Configurable#register(java.lang.String,
     *      edu.cmu.sphinx.util.props.Registry)
     */
    public void register(String name, Registry registry)
            throws PropertyException {
        this.name = name;
        registry.register(PROP_FORMAT, PropertyType.STRING);
        registry.register(PROP_LOCATION, PropertyType.STRING);
        registry.register(PROP_QUERY_LOG_FILE, PropertyType.STRING);
        registry.register(PROP_CACHE_SIZE, PropertyType.INT);
        registry.register(PROP_CLEAR_CACHES_AFTER_UTTERANCE,
                PropertyType.BOOLEAN);
        registry.register(PROP_MAX_DEPTH, PropertyType.INT);
        registry.register(PROP_LOG_MATH, PropertyType.COMPONENT);
        registry.register(PROP_DICTIONARY, PropertyType.COMPONENT);
	registry.register(PROP_DICTIONARY_CLASSE, PropertyType.COMPONENT);
	registry.register("wordLM", PropertyType.COMPONENT);
	registry.register("classeLM", PropertyType.COMPONENT);
	registry.register("basicLM", PropertyType.COMPONENT);
	registry.register("poids",PropertyType.FLOAT);

        registry.register(PROP_APPLY_LANGUAGE_WEIGHT_AND_WIP,
                PropertyType.BOOLEAN);
        registry.register(PROP_LANGUAGE_WEIGHT, PropertyType.FLOAT);
        registry.register(PROP_WORD_INSERTION_PROBABILITY, PropertyType.DOUBLE);
        registry.register(PROP_UNIGRAM_WEIGHT, PropertyType.FLOAT);
        registry.register(PROP_FULL_SMEAR, PropertyType.BOOLEAN);
    }

    /*
     * (non-Javadoc)
     * 
     * @see edu.cmu.sphinx.util.props.Configurable#newProperties(edu.cmu.sphinx.util.props.PropertySheet)
     */
    public void newProperties(PropertySheet ps) throws PropertyException {
        logger = ps.getLogger();
        format = ps.getString(LanguageModel.PROP_FORMAT,
                LanguageModel.PROP_FORMAT_DEFAULT);
        location = new File(ps.getString(PROP_LOCATION, PROP_LOCATION_DEFAULT));
        ngramLogFile = ps.getString(PROP_QUERY_LOG_FILE,
                PROP_QUERY_LOG_FILE_DEFAULT);
        maxCacheSize = ps.getInt(PROP_CACHE_SIZE,
                PROP_CACHE_SIZE_DEFAULT);
        clearCacheAfterUtterance = ps.getBoolean(
                PROP_CLEAR_CACHES_AFTER_UTTERANCE,
                PROP_CLEAR_CACHES_AFTER_UTTERANCE_DEFAULT);
        maxDepth = ps.getInt(LanguageModel.PROP_MAX_DEPTH,
                LanguageModel.PROP_MAX_DEPTH_DEFAULT);
        logMath = (LogMath) ps.getComponent(PROP_LOG_MATH, LogMath.class);
        dictionary = (Dictionary) ps.getComponent(PROP_DICTIONARY,
                Dictionary.class);
        wordLM = (LanguageModel) ps.getComponent("wordLM",
                LanguageModel.class);
       

        applyLanguageWeightAndWip = ps.getBoolean(
                PROP_APPLY_LANGUAGE_WEIGHT_AND_WIP,
                PROP_APPLY_LANGUAGE_WEIGHT_AND_WIP_DEFAULT);
        languageWeight = ps.getFloat(PROP_LANGUAGE_WEIGHT,
                PROP_LANGUAGE_WEIGHT_DEFAULT);
        poids = ps.getFloat("poids",
                0.8f);
	classeLM=null;
	if (poids <1.0) {
	    classeLM = (LanguageModel) ps.getComponent("classeLM",
						       LanguageModel.class);
       dictionaryClasse = (Dictionary) ps.getComponent(PROP_DICTIONARY_CLASSE,
                Dictionary.class);
	}
	
	try { basicLM= (LanguageModel) ps.getComponent("basicLM",
						       LanguageModel.class);
	}
	   catch (Exception e) 
	    { basicLM =null;logger.warning(e.toString());}

        wip = ps.getDouble(PROP_WORD_INSERTION_PROBABILITY,
                PROP_WORD_INSERTION_PROBABILITY_DEFAULT);
        fullSmear = ps.getBoolean(PROP_FULL_SMEAR, PROP_FULL_SMEAR_DEFAULT);
    }

    /*
     * (non-Javadoc)
     * 
     * @see edu.cmu.sphinx.util.props.Configurable#getName()
     */
    public String getName() {
        return name;
    }

    /*
     * (non-Javadoc)
     * 
     * @see edu.cmu.sphinx.linguist.language.ngram.LanguageModel#allocate()
     */
    private int maxDepthWord=0;
    public void allocate() throws IOException {
        Timer.start("LM Load");
	wordLM.allocate();
	maxDepthWord=wordLM.getMaxDepth();
	if (basicLM !=null) {
	    basicLM.allocate();
	}
	else maxDepthWord=0; //pas de recours au basicLM

	if (classeLM==null) {
	  
	    if (maxDepth>wordLM.getMaxDepth())
	    maxDepth=wordLM.getMaxDepth();
	    return;
	}
	
	    
	dictionaryClasse.allocate();
	classeLM.allocate();
	java.io.BufferedReader fichier= new java.io.BufferedReader(
								   new java.io.FileReader(location));
	buildClasse(fichier,dictionary,dictionaryClasse);
        // create the log file if specified
        if (ngramLogFile != null) {
            logFile = new PrintWriter(new FileOutputStream(ngramLogFile));
        }
        cache = new LRUCache(maxCacheSize);
	//        buildUnigramIDMap(dictionary);
	if (wordLM.getMaxDepth()<maxDepth)
	    maxDepth=wordLM.getMaxDepth();
	if (classeLM.getMaxDepth()<maxDepth)
	    maxDepth=classeLM.getMaxDepth();
	
           if (fullSmear) {
	       throw new Error("pas fullsmear fait ");
	       //            System.out.println("Full Smear");
//             try {
//                 System.out.println("... Reading ...");
//                 readSmearInfo("smear.dat");
//                 System.out.println("... Done ");
//             } catch (IOException e) {
//                 System.out.println("... " + e);
//                 System.out.println("... Calculating");
//                 buildSmearInfo();
//                 System.out.println("... Writing");
//                 // writeSmearInfo("smear.dat");
//                 System.out.println("... Done");
//             }
        }
        Timer.stop("LM Load");

    }

    public int getMaxDepth() {
	return maxDepth;
    }
    /*
     * (non-Javadoc)
     * 
     * @see edu.cmu.sphinx.linguist.language.ngram.LanguageModel#deallocate()
     */
    public void deallocate() {
        // TODO write me

    }

    /**
     * Builds the map from unigram to unigramID. Also finds the startWordID and
     * endWordID.
     */
    private void buildClasse(java.io.BufferedReader fichier,
			     Dictionary dico, Dictionary classes) throws IOException   {
	String ligne;
	float proba;
	Word w1,w2;
	String nombre;
	dico.getSentenceStartWord()
	    .setClasse(classes.getSentenceStartWord(),0.0f);
	dico.getSentenceEndWord()
	    .setClasse(classes.getSentenceEndWord(),0.0f);

	while ( (ligne=fichier.readLine())!=null) {
	    Scanner s= new Scanner(ligne);
	    w1= dico.getWord(s.next());
	    w2=classes.getWord(s.next());
	    proba=Float.parseFloat(s.next());
	    proba=logMath.lnToLog(proba);
	    if (w1==null || w2==null) 
		{logger.warning( ligne + "non traite");
		    continue;
		}
	    if (logger.isLoggable(Level.FINE)) 
                logger.fine(String.format("Word: %s classe: %s  p: %f",w1.toString(),w2.toString(),proba));
	    w1.setClasse(w2,proba);
	}
	//dico.dump();
    }
   //  private void buildUnigramIDMap(Dictionary dictionary) {
//         int missingWords = 0;
//         String[] words = loader.getWords();
//         for (int i = 0; i < words.length; i++) {
//             Word word = dictionary.getWord(words[i]);
//             if (word == null) {
//                 logger.info("Missing word: " + words[i]);
//                 missingWords++;
//             }
//             unigramIDMap.put(word, unigrams[i]);
//             if (logger.isLoggable(Level.FINE)) {
//                 logger.fine("Word: " + word);
//             }
//         }

//         if (missingWords > 0) {
//             logger.warning("Dictionary is missing " + missingWords
//                     + " words that are contained in the language model.");
//         }
//     }

    /**
     * Called before a recognition
     */
      public void start() {
	wordLM.start();
	if (basicLM !=null)
	    basicLM.start();
	if (classeLM !=null)
	classeLM.start();
        if (logFile != null) {
            logFile.println("<START_UTT>");
        }
    }

    /**
     * Called after a recognition
     */
    public void stop() {
        clearCache();
	wordLM.stop();
	if (classeLM !=null) classeLM.stop();
	if (basicLM !=null) basicLM.stop();
        if (logFile != null) {
            logFile.println("<END_UTT>");
            logFile.flush();
        }
    }

    /**
     * Clears the various N-gram caches.
     */
    private void clearCache() {
            if (clearCacheAfterUtterance)
		cache = new LRUCache(maxCacheSize);
    }

    /**
     * Gets the ngram probability of the word sequence represented by the word
     * list
     * 
     * @param wordSequence
     *                the word sequence
     * 
     * @return the probability of the word sequence. Probability is in logMath
     *         log base
     *  
     */
    public float getProbability(WordSequence wordSequence) {
	if (classeLM==null)
	    return ((wordSequence.size()<maxDepthWord) ? basicLM.getProbability(wordSequence) : wordLM.getProbability(wordSequence));
	int numberWords = wordSequence.size();
	float ret= -Float.MAX_VALUE;
	float temp;
	float wordProba;
        if (logFile != null) {
            //logFile.println(wordSequence.toText()); 
	    if (numberWords <= maxDepth) {
		Float proba= (Float) cache.get(wordSequence);
		if (proba!=null) ret=proba;
		else {
		    
		    ret= poids*
((wordSequence.size()<maxDepthWord) ? basicLM.getProbability(wordSequence) : wordLM.getProbability(wordSequence));
		    ret +=
			(1.0f-poids)*(classeLM.getProbability(wordSequence.getClasse())+
				      (temp=wordSequence.getNewWord().getClasseProba()));
		    cache.put(wordSequence, ret);
		    logFile.format("%s %.0f %.0f\n",wordSequence.toText(),ret,temp);		 
		}

		if (ret!= -Float.MAX_VALUE) return ret; 
	    }
	    throw new Error("Unsupported N-gram: " + wordSequence.size());
        } else {
	    if (numberWords <= maxDepth) {
		Float proba= (Float) cache.get(wordSequence);
		if (proba!=null) ret=proba;
		else {
		       ret= poids*
((wordSequence.size()<maxDepthWord) ? basicLM.getProbability(wordSequence) : wordLM.getProbability(wordSequence))+
		    
			(1.0f-poids)*(classeLM.getProbability(wordSequence.getClasse())+
				    wordSequence.getNewWord().getClasseProba());
		    cache.put(wordSequence, ret);
		
		    /// a mettre dans le cache et faire le else ..... 
		
		}
		if (ret!= -Float.MAX_VALUE) return ret; 
	    }
	    throw new Error("Unsupported N-gram: " + wordSequence.size());
	}
    }
    
//     public final int getWordID(Word word) {
// 	return wordLM.getWordID(word);}
    /**
     * Gets the smear term for the given wordSequence
     * 
     * @param wordSequence
     *                the word sequence
     * @return the smear term associated with this word sequence
     */
    public float getSmearOld(WordSequence wordSequence) {
        float smearTerm = 0.0f;
//         if (fullSmear) {
//             int length = wordSequence.size();
//             if (length > 0) {
//                 int wordID = getWordID(wordSequence.getWord(length - 1));
//                 smearTerm = (float) unigramSmearTerm[wordID];
//             }
//         }
//         if (fullSmear && logger.isLoggable(Level.FINE)) {
//             logger.fine("SmearTerm: " + smearTerm);
//         }
        return smearTerm;
    }

    int smearCount;
    int smearBigramHit;

    public float getSmear(WordSequence wordSequence) {
        float smearTerm = 1.0f;
//         if (fullSmear) {
//             smearCount++;
//             int length = wordSequence.size();
//             if (length == 1) {
//                 int wordID = getWordID(wordSequence.getWord(0));
//                 smearTerm = (float) unigramSmearTerm[wordID];
//             } else if (length >= 2) {
//                 int size = wordSequence.size();
//                 int wordID1 = getWordID(wordSequence.getWord(size - 2));
//                 int wordID2 = getWordID(wordSequence.getWord(size - 1));
//                 Float st = getSmearTerm(wordID1, wordID2);
//                 if (st == null) {
//                     smearTerm = (float) unigramSmearTerm[wordID2];
//                 } else {
//                     smearTerm = st.floatValue();
//                     smearBigramHit++;
//                 }
//             }

//             if (smearCount % 100000 == 0) {
//                 System.out.println("Smear hit: " + smearBigramHit + 
//                         " tot: " + smearCount);
//             }
//         }
//         if (fullSmear && logger.isLoggable(Level.FINE)) {
//             logger.fine("SmearTerm: " + smearTerm);
//         }
        return smearTerm;
    }


    /**
     * Returns the set of words in the lanaguage model. The set is
     * unmodifiable.
     * 
     * @return the unmodifiable set of words
     */
    public Set getVocabulary() {
        return wordLM.getVocabulary();
    }

    /**
     * Returns the number of times when a bigram is queried, but there is no
     * bigram in the LM (in which case it uses the backoff probabilities).
     * 
     * @return the number of bigram misses
     */
    public int getBigramMisses() {
        return 0;
    }

    /**
     * Returns the number of times when a trigram is queried, but there is no
     * trigram in the LM (in which case it uses the backoff probabilities).
     * 
     * @return the number of trigram misses
     */
    public int getTrigramMisses() {
        return 0;
    }

    /**
     * Returns the number of trigram hits.
     * 
     * @return the number of trigram hits
     */
    public int getTrigramHits() {
        return 0;
    }

}
/**
 * An LRU cache
 */

class LRUCache extends LinkedHashMap {
    int maxSize;

    /**
     * Creates an LRU cache with the given maximum size
     * 
     * @param maxSize
     *                the maximum size of the cache
     */
    LRUCache(int maxSize) {
        this.maxSize = maxSize;
    }

    /**
     * Determines if the eldest entry in the map should be removed.
     * 
     * @param eldest
     *                the eldest entry
     * 
     * @return true if the eldest entry should be removed
     */
    protected boolean removeEldestEntry(Map.Entry eldest) {
        return size() > maxSize;
    }
}
    
